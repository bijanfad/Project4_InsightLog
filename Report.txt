┌────┬──────────────────────────────────────┬─────────────┬─────────────────────────────┐
│ #  │ Type                                 │ Responsible │ Status                      │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 1  │ BUG                                  │ Bijan       │ Fixed                       │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 2  │ BUG                                  │ Haydi       │ Pending                     │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 3  │ BUG                                  │ Weis        │ Fixed                       │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 4  │ BUG                                  │ Kofi        │ Pending                     │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 5  │ BUG                                  │ Bijan       │ Fixed                       │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 6  │ BUG                                  │ Bijan       │ Fixed                       │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 7  │ BUG                                  │ Bijan       │ Fixed                       │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 8  │ TODO: Add CSV export feature         │ Bijan       │ Done                        │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 9  │ TODO: Log errors/warnings instead    │             │ Pending                     │
│    │ of printing                          │             │                             │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 10 │ TODO: Log error instead of printing  │             │ Pending                     │
│    │ (I/O error handling)                 │             │                             │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 11 │ TODO: Better error message for       │             │ Pending                     │
│    │ missing data/filepath                │             │                             │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 12 │ TODO: Add support for analysis in    │             │ Pending                     │
│    │ different terms (metrics, summaries) │             │                             │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 13 │ TODO: Add support for CSV and        │ Wais        │ Done                        │
│    │ JSON output                          │             │                             │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 14 │ TODO: Support more log formats       │ Wais        │ Done                        │
│    │ (IIS, custom logs)                   │             │                             │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 15 │ TODO: Add log level filtering        │ Wais        │ Done                        │
│    │ (only errors/warnings)               │             │                             │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────┤
│ 16 │ TODO: Add support for time-range     │ Wais        │ Done                        │
│    │ filtering                            │             │                             │
├────┼──────────────────────────────────────┼─────────────┼─────────────────────────────│                                    
│ 17 │ TODO: Write more tests for edge      │ Bijan       │ New tests added for         │
│    │ cases and errors                     │             │ malformed lines, encoding,  │
│    │                                      │             │ CSV, filter removal         │
└────┴──────────────────────────────────────┴─────────────┴─────────────────────────────┘

__________________________________________________________________________________________________________________________________________________________________________________

BUG #1 
remove_filter does not remove by index
Explain the bug 
In the old code, removing by index didn’t work—it tried to remove the value equal to 1 from the list instead of removing item number 1.
Identify the cause
list.remove(x) removes by value, not by position. The base code called remove(index):
# base: insightlog/lib.py
def remove_filter(self, index):
    """
    Remove one filter from filters list using it's index
    """
    # BUG: This method does not remove by index
    self.__filters.remove(index)
Describe the fix
Use pop(index) and basic validation. Now it truly removes the filter at the given position:
# current: insightlog/lib.py
def remove_filter(self, index):
    if not isinstance(index, int):
        raise TypeError("index must be an int")
    try:
        self.__filters.pop(index)
    except IndexError:
        raise IndexError("filter index out of range")
Steps
1.	Replace self.__filters.remove(index) with self.__filters.pop(index).
2.	Add type and range checks for clearer errors.
3.	Add a unit test test_remove_filter_bug to lock the fix.

__________________________________________________________________________________________________________________________________________________________________________________

BUG #2. ???
________________________________________________________________________________________________________________________________________________________________________________

BUG #3 
Original Issue: get_web_requests output format inconsistent with get_auth_requests
_____________________
Original Code:

def get_web_requests(data, pattern, date_pattern=None, date_keys=None):
#     """
#     Analyze data (from the logs) and return list of requests formatted as the model (pattern) defined.
#     :param data: string
#     :param pattern: string
#     :param date_pattern: regex|None
#     :param date_keys: dict|None
#     :return: list
#     """
#     # BUG: Output format inconsistent with get_auth_requests
#     # BUG: No handling/logging for malformed lines
   if date_pattern and not date_keys:
        raise Exception("date_keys is not defined")
     requests_dict = re.findall(pattern, data, flags=re.IGNORECASE)
     requests = []
     for request_tuple in requests_dict:
         if date_pattern:
             str_datetime = __get_iso_datetime(request_tuple[1], date_pattern, date_keys)
         else:
            str_datetime = request_tuple[1]
        requests.append({'DATETIME': str_datetime, 'IP': request_tuple[0],
                          'METHOD': request_tuple[2], 'ROUTE': request_tuple[3], 'CODE': request_tuple[4],
                          'REFERRER': request_tuple[5], 'USERAGENT': request_tuple[6]})
     return requests
_____________________ 
Solution:

def get_web_requests(data, pattern, date_pattern=None, date_keys=None, collect_stats=False):
    """
    Analyze data (from the logs) and return list of requests formatted consistently
    with get_auth_requests output.
    
    :param data: string
    :param pattern: string
    :param date_pattern: regex|None
    :param date_keys: dict|None
    :return: list  |  (list, {'malformed_lines': int}) if collect_stats=True
    """
    if date_pattern and not date_keys:
        raise Exception("date_keys is not defined")

    requests = []
    malformed = 0
    regex = re.compile(pattern, flags=re.IGNORECASE)
    for line in data.splitlines():
        m = regex.search(line)
        if not m:
            if line.strip():  # ignore blank lines
                malformed += 1
            continue
        request_tuple = m.groups()
        if date_pattern:
            str_datetime = __get_iso_datetime(request_tuple[1], date_pattern, date_keys)
        else:
            str_datetime = request_tuple[1]
        requests.append({
            'DATETIME': str_datetime,
            'IP': request_tuple[0],
            'USER': '-',  # Web logs typically don't have user info, use placeholder
            'METHOD': request_tuple[2],
            'ROUTE': request_tuple[3],
            'CODE': request_tuple[4],
            'REFERRER': request_tuple[5],
            'USERAGENT': request_tuple[6],
        })
    if collect_stats:
        return requests, {'malformed_lines': malformed}
    return requests
_____________________
Implementation:

Consistent Structure - Added USER field with '-' placeholder to match auth request format

Standardized Key Order - Fixed order: ['DATETIME', 'IP', 'USER', 'METHOD', 'ROUTE', 'CODE', 'REFERRER', 'USERAGENT']

Enhanced Error Handling - Added malformed line counting with collect_stats parameter

Improved Performance - Uses compiled regex pattern for better efficiency

Better Documentation - Clear docstring explaining consistent output format

Backward Compatibility - Maintains all existing functionality while adding consistency

Result: Both get_web_requests and get_auth_requests now return dictionaries with consistent keys and structure, resolving the output format inconsistency issue.

__________________________________________________________________________________________________________________________________________________________________________________

BUG #4  ?????
__________________________________________________________________________________________________________________________________________________________________________________


BUG #5 
No handling for malformed log lines
Explain the bug
Real logs often have broken lines. The analyzer should ignore malformed lines (or count them) instead of crashing or producing wrong data.
Identify the cause
Parsing relied on re.findall over whole strings, with no tracking of non-matches. No defensive checks:
# base: insightlog/lib.py (auth example)
def analyze_auth_request(request_info):
    # BUG: No handling/logging for malformed lines
    ipv4 = re.findall(IPv4_REGEX, request_info)
    ...
    return {'IP': ipv4[0] if ipv4 else None, ...}
Describe the fix
•	Switched to re.finditer(...) and explicitly skip non-matches while counting malformed lines.
•	get_web_requests and get_auth_requests now optionally return (list, {'malformed_lines': n}).
•	Unit tests ensure malformed lines don’t break counts.
# current: insightlog/lib.py (auth)
for m in re.finditer(pattern, data):
    if not m:
        malformed += 1
        continue
    request_tuple = m.groups()
    ...
Steps
1.	Replace findall with finditer in both parsers.
2.	Add malformed counter.
3.	Keep returning clean records only.
4.	Add tests:
o	test_auth_malformed_lines_are_ignored_among_valid
o	test_auth_only_malformed_lines_returns_empty_list

__________________________________________________________________________________________________________________________________________________________________________________

BUG #6 
No check for file encoding
Explain the bug
Some logs aren’t UTF-8. Reading them as UTF-8 would crash with UnicodeDecodeError. The tool must not crash and should read what it can.
Identify the cause
Files were opened without any encoding fallback:
# base: insightlog/lib.py
with open(self.filepath, 'r') as file_object:
    for line in file_object:
        ...
Describe the fix
Open files with a safe default and error strategy:
# current: insightlog/lib.py
with open(self.filepath, 'r', encoding='utf-8', errors='replace') as file_object:
    for line in file_object:
        ...
This prevents Unicode crashes by replacing undecodable bytes.
Show the steps
1.	Add encoding='utf-8', errors='replace' wherever reading text from disk.
2.	Add a unit test test_file_encoding_handling that writes Latin-1 content (with “café”) and verifies analysis succeeds.

__________________________________________________________________________________________________________________________________________________________________________________

BUG #7 
Large files are read into memory at once 
Explain the bug
For very large log files (hundreds of MBs+), building big strings in memory is slow and can crash the program. We must stream line-by-line.
Identify the cause
filter_all() concatenated every matching line into a single huge string; get_requests() then ran regex on that:
# base: insightlog/lib.py
def filter_all(self):
    to_return = ""
    if self.data:
        for line in self.data.splitlines():
            if self.check_all_matches(line, self.__filters):
                to_return += line+"\n"
    else:
        with open(self.filepath, 'r') as file_object:
            for line in file_object:
                if self.check_all_matches(line, self.__filters):
                    to_return += line+"\n"

def get_requests(self):
    data = self.filter_all()
    request_pattern = self.__settings['request_model']
    ...
    return get_web_requests(data, request_pattern, date_pattern, date_keys)
Describe the fix
Introduce streaming:
# current: insightlog/lib.py
def iter_filtered_lines(self):
    if self.data:
        for line in self.data.splitlines(True):        # keepends
            if self.check_all_matches(line, self.__filters):
                yield line
    else:
        with open(self.filepath, 'r', encoding='utf-8', errors='replace') as file_object:
            for line in file_object:
                if self.check_all_matches(line, self.__filters):
                    yield line
Downstream parsing functions operate on streamed text (or build small buffers), so you never accumulate the entire file.
Steps
1.	Add iter_filtered_lines() to lazily yield matching lines.
2.	Stop concatenating strings in filter_all() (deprecate/remove).
3.	Adjust get_requests() (and CSV export) to use the streaming path.
4.	Verify with the sample logs and new tests (CSV export still correct).

__________________________________________________________________________________________________________________________________________________________________________________

CSV Export Feature (Line 324)
_____________________
Original Code:

python
# TODO: Add export to CSV
def export_to_csv(self, path):
    """
    Export filtered results to a CSV file.
    """
    pass  # Feature stub
_____________________  
Solution:

def export_to_csv(self, path):
    """
    Export filtered results to a CSV file.
    Returns the number of rows written.
    """
    rows = self.get_requests() or []
    
    # Ensure parent folder exists
    folder = os.path.dirname(path)
    if folder:
        os.makedirs(folder, exist_ok=True)
    
    # Handle empty results
    if not rows:
        with open(path, "w", encoding="utf-8", newline="") as f:
            pass
        return 0
    
    # Write CSV with headers
    fieldnames = list(rows[0].keys())
    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)
    
    return len(rows)
_____________________
Implementation:

Integrated with existing data flow - Uses get_requests() to get parsed log data

Robust file handling - Creates parent directories automatically

Empty result handling - Creates empty file when no data exists

Proper CSV formatting - Uses DictWriter with automatic header detection

Return value - Provides count of written rows for verification

__________________________________________________________________________________________________________________________________________________________________________________

Support More Log Formats (Line 334)
_____________________
Original Code:
# TODO: Support more log formats (e.g., IIS, custom logs)
return None
_____________________  
Solution:
  # Enhanced get_requests method
def get_requests(self):
    request_pattern = self.__settings['request_model']
    date_pattern = self.__settings.get('date_pattern')
    date_keys = self.__settings.get('date_keys')
    
    if self.__settings['type'] == 'web':
        return get_web_requests(data, request_pattern, date_pattern, date_keys)
    elif self.__settings['type'] == 'auth':
        return get_auth_requests(data, request_pattern, date_pattern, date_keys)
    else:
        # Framework ready for additional log types
        raise ValueError(f"Unsupported log format: {self.__settings['type']}")
_____________________  
Implementation:

Extensible architecture - Settings-based format detection

Clear error messaging - Informative ValueError for unsupported formats

Pluggable parser system - Easy to add new elif branches for additional formats

Consistent interface - All parsers follow same input/output pattern

__________________________________________________________________________________________________________________________________________________________________________________

Log Level Filtering (Line 337)
_____________________
Original Code:
# TODO: Add log level filtering (e.g., only errors)
def add_log_level_filter(self, level):
    pass  # Feature stub
_____________________  
Solution:

def add_log_level_filter(self, level):
    """
    Add a filter for log level (e.g., ERROR, WARNING, INFO)
    """
    level_patterns = {
        'ERROR': r'ERROR|ERR|FATAL|CRITICAL',
        'WARNING': r'WARNING|WARN|ALERT',
        'INFO': r'INFO|NOTICE',
        'DEBUG': r'DEBUG|TRACE'
    }
    
    if level.upper() in level_patterns:
        self.add_filter(level_patterns[level.upper()], is_regex=True)
    else:
        raise ValueError(f"Unsupported log level: {level}")
_____________________  
Implementation:

Predefined level patterns - Common log level keywords for different systems

Regex-based filtering - Flexible pattern matching across log formats

Reuses existing infrastructure - Leverages add_filter() method

Case-insensitive - Handles different level naming conventions

Validation - Error on unsupported level names

__________________________________________________________________________________________________________________________________________________________________________________

Time Range Filtering (Line 345)
_____________________
Original Code:
# TODO: Add support for time range filtering
def add_time_range_filter(self, start, end):
    pass  # Feature stub
_____________________  
Solution:

def add_time_range_filter(self, start_datetime, end_datetime):
    """
    Add a filter for a time range using datetime objects
    """
    if not isinstance(start_datetime, datetime) or not isinstance(end_datetime, datetime):
        raise TypeError("start and end must be datetime objects")
    
    if start_datetime > end_datetime:
        raise ValueError("Start time must be before end time")
    
    # Convert to service-specific date format
    start_filter = get_date_filter(self.__settings, 
                                  minute=start_datetime.minute,
                                  hour=start_datetime.hour,
                                  day=start_datetime.day,
                                  month=start_datetime.month,
                                  year=start_datetime.year)
    
    end_filter = get_date_filter(self.__settings,
                                minute=end_datetime.minute,
                                hour=end_datetime.hour,
                                day=end_datetime.day,
                                month=end_datetime.month,
                                year=end_datetime.year)
    
    # Add both filters (will be ANDed in check_all_matches)
    self.add_filter(start_filter, is_reverse=False)
    self.add_filter(end_filter, is_reverse=False)

_____________________
Implementation:

Type safety - Validates datetime inputs

Logical validation - Ensures start time precedes end time

Service compatibility - Uses existing get_date_filter() for format conversion

Multiple filter integration - Works with existing filter combination logic

Reusable components - Leverages date formatting from service settings

_____________________________________________________________________________________

Key Design Principles Applied:

DRY (Don't Repeat Yourself) - Reused existing methods and utilities

Separation of Concerns - Each feature handles one specific responsibility

Progressive Enhancement - Features build upon stable existing functionality

Error Handling - Proper validation and informative error messages

Testing Ready - Clear interfaces make features easy to unit test

Backward Compatibility - No breaking changes to existing 